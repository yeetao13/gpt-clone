# GPT-Clone

A simple GPT-style language model project inspired by Sebastian Raschkaâ€™s tutorial: _Build a Large Language Model From Scratch_.

This project walks through training and fine-tuning a GPT-2 model from scratch on a small dataset, and adapting it for various NLP tasks.

---

## ðŸ”§ What I Did

- **Trained a simpler version of GPT-2 model** from scratch to predict tokens using a small text corpus.
- **Fine-tuned the model for text classification** using labeled datasets.
- **Adapted the model for instruction-following** using prompt-based inputs.
- **Implemented LoRA (Low-Rank Adaptation)** for efficient fine-tuning with fewer parameters.

---

## ðŸ§  Goals

- Understand how decoder-only transformers like GPT-2 work.
- Learn how to fine-tune LLMs for specific tasks.
- Practice efficient training techniques like LoRA.

---

## ðŸ“š Reference

- Build a Large Language Model From Scratch from Sebastian Raschka

---

